{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f210cc",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "This file is for evaluation, the training and test sample generations are done in the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline configuration\n",
    "# Configuration name: 3d_fullres\n",
    "# {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [64, 128, 192], 'median_image_size_in_voxels': [59.0, 117.0, 180.5], 'spacing': [2.0, 0.732421875, 0.732421875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False}\n",
    "# These are the global plan.json settings:\n",
    "# {'dataset_name': 'Dataset001_PancreasSegClassification', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.0, 0.732421875, 0.732421875], 'original_median_shape_after_transp': [64, 119, 179], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1929.0, 'mean': 74.06402587890625, 'median': 77.98674774169922, 'min': -406.9988098144531, 'percentile_00_5': -56.0, 'percentile_99_5': 179.99807739257812, 'std': 44.359100341796875}}}\n",
    "# trained for 198 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4806a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 72 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 72 cases that I would like to predict\n",
      "\n",
      "Predicting quiz_037:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_037\n",
      "\n",
      "Predicting quiz_045:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_045\n",
      "\n",
      "Predicting quiz_047:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_047\n",
      "\n",
      "Predicting quiz_048:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_048\n",
      "\n",
      "Predicting quiz_052:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_052\n",
      "\n",
      "Predicting quiz_053:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_053\n",
      "\n",
      "Predicting quiz_056:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_056\n",
      "\n",
      "Predicting quiz_068:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_068\n",
      "\n",
      "Predicting quiz_069:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_069\n",
      "\n",
      "Predicting quiz_092:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_092\n",
      "\n",
      "Predicting quiz_095:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_095\n",
      "\n",
      "Predicting quiz_097:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_097\n",
      "\n",
      "Predicting quiz_101:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_101\n",
      "\n",
      "Predicting quiz_108:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_108\n",
      "\n",
      "Predicting quiz_129:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_129\n",
      "\n",
      "Predicting quiz_130:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_130\n",
      "\n",
      "Predicting quiz_135:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_135\n",
      "\n",
      "Predicting quiz_137:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_137\n",
      "\n",
      "Predicting quiz_141:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_141\n",
      "\n",
      "Predicting quiz_144:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_144\n",
      "\n",
      "Predicting quiz_146:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_146\n",
      "\n",
      "Predicting quiz_152:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_152\n",
      "\n",
      "Predicting quiz_155:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_155\n",
      "\n",
      "Predicting quiz_157:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_157\n",
      "\n",
      "Predicting quiz_161:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_161\n",
      "\n",
      "Predicting quiz_162:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_162\n",
      "\n",
      "Predicting quiz_226:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_226\n",
      "\n",
      "Predicting quiz_228:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_228\n",
      "\n",
      "Predicting quiz_229:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_229\n",
      "\n",
      "Predicting quiz_233:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_233\n",
      "\n",
      "Predicting quiz_238:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_238\n",
      "\n",
      "Predicting quiz_257:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_257\n",
      "\n",
      "Predicting quiz_261:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_261\n",
      "\n",
      "Predicting quiz_263:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_263\n",
      "\n",
      "Predicting quiz_266:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_266\n",
      "\n",
      "Predicting quiz_267:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_267\n",
      "\n",
      "Predicting quiz_269:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_269\n",
      "\n",
      "Predicting quiz_270:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_270\n",
      "\n",
      "Predicting quiz_271:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_271\n",
      "\n",
      "Predicting quiz_275:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_275\n",
      "\n",
      "Predicting quiz_280:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_280\n",
      "\n",
      "Predicting quiz_282:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_282\n",
      "\n",
      "Predicting quiz_347:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_347\n",
      "\n",
      "Predicting quiz_349:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_349\n",
      "\n",
      "Predicting quiz_351:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_351\n",
      "\n",
      "Predicting quiz_353:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_353\n",
      "\n",
      "Predicting quiz_357:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_357\n",
      "\n",
      "Predicting quiz_361:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_361\n",
      "\n",
      "Predicting quiz_367:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_367\n",
      "\n",
      "Predicting quiz_369:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_369\n",
      "\n",
      "Predicting quiz_371:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_371\n",
      "\n",
      "Predicting quiz_373:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_373\n",
      "\n",
      "Predicting quiz_376:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_376\n",
      "\n",
      "Predicting quiz_378:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_378\n",
      "\n",
      "Predicting quiz_388:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_388\n",
      "\n",
      "Predicting quiz_391:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_391\n",
      "\n",
      "Predicting quiz_402:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_402\n",
      "\n",
      "Predicting quiz_405:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_405\n",
      "\n",
      "Predicting quiz_408:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_408\n",
      "\n",
      "Predicting quiz_409:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_409\n",
      "\n",
      "Predicting quiz_417:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_417\n",
      "\n",
      "Predicting quiz_422:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_422\n",
      "\n",
      "Predicting quiz_424:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_424\n",
      "\n",
      "Predicting quiz_425:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_425\n",
      "\n",
      "Predicting quiz_428:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_428\n",
      "\n",
      "Predicting quiz_429:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_429\n",
      "\n",
      "Predicting quiz_499:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_499\n",
      "\n",
      "Predicting quiz_504:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_504\n",
      "\n",
      "Predicting quiz_512:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_512\n",
      "\n",
      "Predicting quiz_520:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_520\n",
      "\n",
      "Predicting quiz_521:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_521\n",
      "\n",
      "Predicting quiz_524:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with quiz_524\n",
      "\n",
      "STDERR: \n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:06,  1.03it/s]\n",
      " 25%|██▌       | 2/8 [00:01<00:02,  2.13it/s]\n",
      " 38%|███▊      | 3/8 [00:01<00:01,  2.81it/s]\n",
      " 50%|█████     | 4/8 [00:01<00:01,  3.32it/s]\n",
      " 62%|██████▎   | 5/8 [00:01<00:00,  3.71it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  3.98it/s]\n",
      " 88%|████████▊ | 7/8 [00:02<00:00,  4.18it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  4.32it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.35it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.61it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.76it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.23it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.56it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.09it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.90it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.81it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.75it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.72it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.69it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.76it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.25it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.54it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.08it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.91it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.81it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.75it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.71it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.69it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.77it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.85it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.21it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.47it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.02it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.86it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.78it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.73it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.70it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.87it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.87it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.30it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.57it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.44it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.87it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.85it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.51it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.80it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.34it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.50it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.57it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.59it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.57it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.08it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.89it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.80it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.74it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.71it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.21it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.48it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.02it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.86it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.78it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.73it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.70it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.86it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.87it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.26it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.55it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.60it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.57it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.89it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.46it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.55it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.34it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.48it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.55it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.57it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.33it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.45it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.55it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.57it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.57it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.22it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.51it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.14it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.52it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.54it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.15it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.23it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.52it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.14it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.53it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.56it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.07it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.89it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.79it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.73it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.69it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.67it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.88it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.56it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.80it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.19it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.53it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.15it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  8.27it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00,  5.56it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.23it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:01,  5.54it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00,  5.07it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00,  4.88it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00,  4.78it/s]\n",
      " 75%|███████▌  | 6/8 [00:01<00:00,  4.72it/s]\n",
      " 88%|████████▊ | 7/8 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.66it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.86it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "env_vars = os.environ.copy()\n",
    "# print(root_dir)\n",
    "env_vars['nnUNet_raw'] = \"/mnt/data/gpu-server/m31_nnUnet/nnunet_data/nnUNet_raw\"\n",
    "env_vars['nnUNet_preprocessed'] = \"/mnt/data/gpu-server/m31_nnUnet/nnunet_data/nnUNet_preprocessed\"\n",
    "env_vars['nnUNet_results'] = \"/mnt/data/gpu-server/m31_nnUnet/nnunet_data/nnUNet_results\"\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"uv\", \"run\", \"--extra\", \"cu124\",\n",
    "    \"nnUNetv2_predict\",\n",
    "    # \"-pl\", \"nnUNetPlannerResEncM\", # use without to use default planner\n",
    "    \"-i\", \"/mnt/data/gpu-server/m31_nnUnet/nnunet_data/nnUNet_raw/Dataset001_PancreasSegClassification/imagesTs\",\n",
    "    \"-o\", \"/mnt/data/gpu-server/m31_nnUnet/nnunet_data/nnUNet_results/val_predictions\",\n",
    "    \"-d\", \"1\",\n",
    "    \"-c\", \"3d_fullres\",\n",
    "    \"-f\", \"0\",\n",
    "    \"-chk\", \"checkpoint_best.pth\",\n",
    "], env=env_vars, capture_output=True, text=True, check=True)\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25722a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths - MODIFY THESE ACCORDING TO YOUR SETUP\n",
    "PRED_FOLDER = \"nnunet_data/nnUNet_results/test_predictions\"  # Your prediction folder\n",
    "GT_VALIDATION_FOLDER = \"./data/validation\"  # Ground truth validation folder\n",
    "OUTPUT_DIR = \"./evaluation_results\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Predictions folder: {PRED_FOLDER}\")\n",
    "print(f\"Ground truth folder: {GT_VALIDATION_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfe30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice_coefficient(y_true, y_pred):\n",
    "    \"\"\"Compute Dice coefficient between two binary masks\"\"\"\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    if union == 0:\n",
    "        return 1.0 if np.sum(y_true) == np.sum(y_pred) == 0 else 0.0\n",
    "    return (2.0 * intersection) / union\n",
    "\n",
    "def compute_hausdorff_distance(y_true, y_pred, spacing=(1.0, 1.0, 1.0)):\n",
    "    \"\"\"Compute Hausdorff distance between two binary masks\"\"\"\n",
    "    try:\n",
    "        # Get surface points using erosion\n",
    "        true_surface = np.argwhere(ndimage.binary_erosion(y_true) != y_true)\n",
    "        pred_surface = np.argwhere(ndimage.binary_erosion(y_pred) != y_pred)\n",
    "\n",
    "        if len(true_surface) == 0 or len(pred_surface) == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        # Apply spacing\n",
    "        true_surface = true_surface * np.array(spacing)\n",
    "        pred_surface = pred_surface * np.array(spacing)\n",
    "\n",
    "        # Compute directed Hausdorff distances\n",
    "        hd1 = directed_hausdorff(true_surface, pred_surface)[0]\n",
    "        hd2 = directed_hausdorff(pred_surface, true_surface)[0]\n",
    "\n",
    "        return max(hd1, hd2)\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def compute_volume_metrics(y_true, y_pred, spacing=(1.0, 1.0, 1.0)):\n",
    "    \"\"\"Compute volume-based metrics\"\"\"\n",
    "    voxel_volume = np.prod(spacing)\n",
    "    true_volume = np.sum(y_true) * voxel_volume\n",
    "    pred_volume = np.sum(y_pred) * voxel_volume\n",
    "\n",
    "    if true_volume == 0:\n",
    "        volume_error = pred_volume\n",
    "    else:\n",
    "        volume_error = abs(pred_volume - true_volume) / true_volume * 100\n",
    "\n",
    "    return true_volume, pred_volume, volume_error\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth CSV for validation set if it doesn't exist\n",
    "def create_validation_gt_csv():\n",
    "    gt_csv_path = Path(OUTPUT_DIR) / \"validation_ground_truth.csv\"\n",
    "\n",
    "    if gt_csv_path.exists():\n",
    "        print(f\"Ground truth CSV already exists: {gt_csv_path}\")\n",
    "        return gt_csv_path\n",
    "\n",
    "    # Create ground truth CSV from folder structure\n",
    "    validation_data = []\n",
    "\n",
    "    for subtype in ['subtype0', 'subtype1', 'subtype2']:\n",
    "        subtype_folder = Path(GT_VALIDATION_FOLDER) / subtype\n",
    "        if subtype_folder.exists():\n",
    "            for mask_file in subtype_folder.glob(\"*.nii.gz\"):\n",
    "                if not mask_file.name.endswith(\"_0000.nii.gz\"):  # Skip image files\n",
    "                    case_name = mask_file.name\n",
    "                    subtype_id = int(subtype.replace('subtype', ''))\n",
    "                    validation_data.append({'Names': case_name, 'Subtype': subtype_id})\n",
    "\n",
    "    if validation_data:\n",
    "        df = pd.DataFrame(validation_data)\n",
    "        df.to_csv(gt_csv_path, index=False)\n",
    "        print(f\"Created ground truth CSV with {len(df)} cases: {gt_csv_path}\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No validation data found in the specified folder structure\")\n",
    "        return None\n",
    "\n",
    "    return gt_csv_path\n",
    "\n",
    "# Create or load ground truth CSV\n",
    "gt_csv_path = create_validation_gt_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation_performance():\n",
    "    \"\"\"Evaluate segmentation performance on validation set\"\"\"\n",
    "    results = []\n",
    "    pred_folder = Path(PRED_FOLDER)\n",
    "\n",
    "    # Find all prediction files\n",
    "    pred_files = list(pred_folder.glob(\"*.nii.gz\"))\n",
    "    print(f\"Found {len(pred_files)} prediction files\")\n",
    "\n",
    "    for pred_file in pred_files:\n",
    "        case_id = pred_file.stem.replace('.nii', '')\n",
    "\n",
    "        # Find corresponding ground truth file\n",
    "        gt_file = None\n",
    "        for subtype in ['subtype0', 'subtype1', 'subtype2']:\n",
    "            potential_gt = Path(GT_VALIDATION_FOLDER) / subtype / f\"{case_id}.nii.gz\"\n",
    "            if potential_gt.exists():\n",
    "                gt_file = potential_gt\n",
    "                break\n",
    "\n",
    "        if gt_file is None:\n",
    "            print(f\"Warning: No ground truth found for {case_id}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            gt_img = nib.load(gt_file)\n",
    "            pred_img = nib.load(pred_file)\n",
    "\n",
    "            gt_data = gt_img.get_fdata().astype(np.uint8)\n",
    "            pred_data = pred_img.get_fdata().astype(np.uint8)\n",
    "\n",
    "            # Get spacing for physical distance metrics\n",
    "            spacing = gt_img.header.get_zooms()\n",
    "\n",
    "            # Whole pancreas (label 1 + label 2)\n",
    "            gt_pancreas = (gt_data > 0).astype(np.uint8)\n",
    "            pred_pancreas = (pred_data > 0).astype(np.uint8)\n",
    "\n",
    "            # Lesion only (label 2)\n",
    "            gt_lesion = (gt_data == 2).astype(np.uint8)\n",
    "            pred_lesion = (pred_data == 2).astype(np.uint8)\n",
    "\n",
    "            # Compute metrics\n",
    "            pancreas_dice = compute_dice_coefficient(gt_pancreas, pred_pancreas)\n",
    "            lesion_dice = compute_dice_coefficient(gt_lesion, pred_lesion)\n",
    "\n",
    "            # Volume metrics\n",
    "            pancreas_vol_gt, pancreas_vol_pred, pancreas_vol_error = compute_volume_metrics(\n",
    "                gt_pancreas, pred_pancreas, spacing)\n",
    "            lesion_vol_gt, lesion_vol_pred, lesion_vol_error = compute_volume_metrics(\n",
    "                gt_lesion, pred_lesion, spacing)\n",
    "\n",
    "            # Hausdorff distance (optional - can be slow)\n",
    "            # pancreas_hd95 = compute_hausdorff_distance(gt_pancreas, pred_pancreas, spacing)\n",
    "            # lesion_hd95 = compute_hausdorff_distance(gt_lesion, pred_lesion, spacing)\n",
    "\n",
    "            results.append({\n",
    "                'case_id': case_id,\n",
    "                'pancreas_dice': pancreas_dice,\n",
    "                'lesion_dice': lesion_dice,\n",
    "                'pancreas_vol_gt': pancreas_vol_gt,\n",
    "                'pancreas_vol_pred': pancreas_vol_pred,\n",
    "                'pancreas_vol_error': pancreas_vol_error,\n",
    "                'lesion_vol_gt': lesion_vol_gt,\n",
    "                'lesion_vol_pred': lesion_vol_pred,\n",
    "                'lesion_vol_error': lesion_vol_error,\n",
    "                # 'pancreas_hd95': pancreas_hd95,\n",
    "                # 'lesion_hd95': lesion_hd95\n",
    "            })\n",
    "\n",
    "            print(f\"{case_id}: Pancreas DSC={pancreas_dice:.3f}, Lesion DSC={lesion_dice:.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {case_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run segmentation evaluation\n",
    "seg_results_df = evaluate_segmentation_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(seg_results_df) > 0:\n",
    "    # Compute summary statistics\n",
    "    seg_summary = {\n",
    "        'num_cases': len(seg_results_df),\n",
    "        'mean_pancreas_dice': seg_results_df['pancreas_dice'].mean(),\n",
    "        'std_pancreas_dice': seg_results_df['pancreas_dice'].std(),\n",
    "        'median_pancreas_dice': seg_results_df['pancreas_dice'].median(),\n",
    "        'mean_lesion_dice': seg_results_df['lesion_dice'].mean(),\n",
    "        'std_lesion_dice': seg_results_df['lesion_dice'].std(),\n",
    "        'median_lesion_dice': seg_results_df['lesion_dice'].median(),\n",
    "    }\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"SEGMENTATION RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of cases: {seg_summary['num_cases']}\")\n",
    "    print(f\"Whole pancreas DSC: {seg_summary['mean_pancreas_dice']:.4f} ± {seg_summary['std_pancreas_dice']:.4f}\")\n",
    "    print(f\"Whole pancreas DSC (median): {seg_summary['median_pancreas_dice']:.4f}\")\n",
    "    print(f\"Pancreas lesion DSC: {seg_summary['mean_lesion_dice']:.4f} ± {seg_summary['std_lesion_dice']:.4f}\")\n",
    "    print(f\"Pancreas lesion DSC (median): {seg_summary['median_lesion_dice']:.4f}\")\n",
    "\n",
    "    # Check targets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TARGET ASSESSMENT - SEGMENTATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Master level targets\n",
    "    pancreas_target = 0.91\n",
    "    lesion_target = 0.31\n",
    "\n",
    "    pancreas_met = seg_summary['mean_pancreas_dice'] >= pancreas_target\n",
    "    lesion_met = seg_summary['mean_lesion_dice'] >= lesion_target\n",
    "\n",
    "    print(f\"Whole pancreas DSC >= {pancreas_target}: {'✓' if pancreas_met else '✗'} \"\n",
    "          f\"({seg_summary['mean_pancreas_dice']:.4f})\")\n",
    "    print(f\"Lesion DSC >= {lesion_target}: {'✓' if lesion_met else '✗'} \"\n",
    "          f\"({seg_summary['mean_lesion_dice']:.4f})\")\n",
    "\n",
    "    # Save detailed results\n",
    "    seg_results_df.to_csv(Path(OUTPUT_DIR) / 'segmentation_detailed_results.csv', index=False)\n",
    "\n",
    "    with open(Path(OUTPUT_DIR) / 'segmentation_summary.json', 'w') as f:\n",
    "        json.dump(seg_summary, f, indent=2)\n",
    "\n",
    "    print(f\"\\nDetailed results saved to: {Path(OUTPUT_DIR) / 'segmentation_detailed_results.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"No segmentation results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(seg_results_df) > 0:\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Dice score distributions\n",
    "    axes[0, 0].hist(seg_results_df['pancreas_dice'], bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0, 0].axvline(seg_results_df['pancreas_dice'].mean(), color='red', linestyle='--',\n",
    "                       label=f'Mean: {seg_results_df[\"pancreas_dice\"].mean():.3f}')\n",
    "    axes[0, 0].axvline(0.91, color='green', linestyle='-', label='Target: 0.91')\n",
    "    axes[0, 0].set_title('Whole Pancreas Dice Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Dice Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0, 1].hist(seg_results_df['lesion_dice'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[0, 1].axvline(seg_results_df['lesion_dice'].mean(), color='red', linestyle='--',\n",
    "                       label=f'Mean: {seg_results_df[\"lesion_dice\"].mean():.3f}')\n",
    "    axes[0, 1].axvline(0.31, color='green', linestyle='-', label='Target: 0.31')\n",
    "    axes[0, 1].set_title('Lesion Dice Score Distribution')\n",
    "    axes[0, 1].set_xlabel('Dice Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Box plots\n",
    "    dice_data = [seg_results_df['pancreas_dice'], seg_results_df['lesion_dice']]\n",
    "    axes[1, 0].boxplot(dice_data, labels=['Whole Pancreas', 'Lesion'])\n",
    "    axes[1, 0].set_title('Dice Score Box Plots')\n",
    "    axes[1, 0].set_ylabel('Dice Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Scatter plot\n",
    "    axes[1, 1].scatter(seg_results_df['pancreas_dice'], seg_results_df['lesion_dice'], alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Whole Pancreas Dice')\n",
    "    axes[1, 1].set_ylabel('Lesion Dice')\n",
    "    axes[1, 1].set_title('Pancreas vs Lesion Dice Correlation')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(OUTPUT_DIR) / 'segmentation_results_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nDETAILED STATISTICS:\")\n",
    "    print(seg_results_df[['pancreas_dice', 'lesion_dice']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_performance(pred_csv_path):\n",
    "    \"\"\"Evaluate classification performance\"\"\"\n",
    "\n",
    "    if not Path(pred_csv_path).exists():\n",
    "        print(f\"Classification predictions file not found: {pred_csv_path}\")\n",
    "        print(\"Please create a CSV file with columns: ['Names', 'Subtype']\")\n",
    "        return None\n",
    "\n",
    "    if gt_csv_path is None:\n",
    "        print(\"Ground truth CSV not available\")\n",
    "        return None\n",
    "\n",
    "    # Load ground truth and predictions\n",
    "    gt_df = pd.read_csv(gt_csv_path)\n",
    "    pred_df = pd.read_csv(pred_csv_path)\n",
    "\n",
    "    print(f\"Ground truth shape: {gt_df.shape}\")\n",
    "    print(f\"Predictions shape: {pred_df.shape}\")\n",
    "\n",
    "    # Merge on case names\n",
    "    merged = pd.merge(gt_df, pred_df, on='Names', suffixes=('_gt', '_pred'))\n",
    "    print(f\"Merged shape: {merged.shape}\")\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        print(\"No matching cases found between ground truth and predictions\")\n",
    "        return None\n",
    "\n",
    "    y_true = merged['Subtype_gt'].values\n",
    "    y_pred = merged['Subtype_pred'].values\n",
    "\n",
    "    # Compute metrics\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Detailed classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of cases: {len(merged)}\")\n",
    "    print(f\"Macro-average F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro-average F1: {micro_f1:.4f}\")\n",
    "    print(f\"Weighted-average F1: {weighted_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class results:\")\n",
    "    for class_id in ['0', '1', '2']:\n",
    "        if class_id in report:\n",
    "            f1 = report[class_id]['f1-score']\n",
    "            precision = report[class_id]['precision']\n",
    "            recall = report[class_id]['recall']\n",
    "            support = report[class_id]['support']\n",
    "            print(f\"  Subtype {class_id}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}, N={support}\")\n",
    "\n",
    "    # Check targets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TARGET ASSESSMENT - CLASSIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    f1_target = 0.7\n",
    "    f1_met = macro_f1 >= f1_target\n",
    "\n",
    "    print(f\"Macro F1 >= {f1_target}: {'✓' if f1_met else '✗'} ({macro_f1:.4f})\")\n",
    "\n",
    "    results = {\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'classification_report': report,\n",
    "        'num_cases': len(merged),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    with open(Path(OUTPUT_DIR) / 'classification_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results, merged\n",
    "\n",
    "# Example usage - MODIFY THE PATH TO YOUR CLASSIFICATION PREDICTIONS\n",
    "classification_pred_csv = \"./subtype_results.csv\"  # UPDATE THIS PATH\n",
    "\n",
    "if Path(classification_pred_csv).exists():\n",
    "    class_results, class_merged_df = evaluate_classification_performance(classification_pred_csv)\n",
    "else:\n",
    "    print(f\"Classification predictions not found at: {classification_pred_csv}\")\n",
    "    print(\"Please provide the path to your classification results CSV file\")\n",
    "    class_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if class_results is not None:\n",
    "    # Create confusion matrix plot\n",
    "    y_true = class_merged_df['Subtype_gt'].values\n",
    "    y_pred = class_merged_df['Subtype_pred'].values\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Subtype 0', 'Subtype 1', 'Subtype 2'],\n",
    "                yticklabels=['Subtype 0', 'Subtype 1', 'Subtype 2'])\n",
    "    plt.title('Classification Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(Path(OUTPUT_DIR) / 'classification_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred,\n",
    "                              target_names=['Subtype 0', 'Subtype 1', 'Subtype 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Segmentation summary\n",
    "if len(seg_results_df) > 0:\n",
    "    print(\"\\nSEGMENTATION PERFORMANCE:\")\n",
    "    print(f\"  Whole pancreas DSC: {seg_results_df['pancreas_dice'].mean():.4f} ± {seg_results_df['pancreas_dice'].std():.4f}\")\n",
    "    print(f\"  Lesion DSC: {seg_results_df['lesion_dice'].mean():.4f} ± {seg_results_df['lesion_dice'].std():.4f}\")\n",
    "\n",
    "# Classification summary\n",
    "if class_results is not None:\n",
    "    print(f\"\\nCLASSIFICATION PERFORMANCE:\")\n",
    "    print(f\"  Macro F1-score: {class_results['macro_f1']:.4f}\")\n",
    "    print(f\"  Micro F1-score: {class_results['micro_f1']:.4f}\")\n",
    "\n",
    "# Target assessment\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET ACHIEVEMENT (Master Level)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "targets_met = []\n",
    "\n",
    "if len(seg_results_df) > 0:\n",
    "    pancreas_target = 0.91\n",
    "    lesion_target = 0.31\n",
    "\n",
    "    pancreas_met = seg_results_df['pancreas_dice'].mean() >= pancreas_target\n",
    "    lesion_met = seg_results_df['lesion_dice'].mean() >= lesion_target\n",
    "\n",
    "    print(f\"✓ Whole pancreas DSC >= {pancreas_target}: {'PASS' if pancreas_met else 'FAIL'} \"\n",
    "          f\"({seg_results_df['pancreas_dice'].mean():.4f})\")\n",
    "    print(f\"✓ Lesion DSC >= {lesion_target}: {'PASS' if lesion_met else 'FAIL'} \"\n",
    "          f\"({seg_results_df['lesion_dice'].mean():.4f})\")\n",
    "\n",
    "    targets_met.extend([pancreas_met, lesion_met])\n",
    "\n",
    "if class_results is not None:\n",
    "    f1_target = 0.7\n",
    "    f1_met = class_results['macro_f1'] >= f1_target\n",
    "\n",
    "    print(f\"✓ Macro F1 >= {f1_target}: {'PASS' if f1_met else 'FAIL'} \"\n",
    "          f\"({class_results['macro_f1']:.4f})\")\n",
    "\n",
    "    targets_met.append(f1_met)\n",
    "\n",
    "if targets_met:\n",
    "    all_met = all(targets_met)\n",
    "    print(f\"\\n🎯 ALL MASTER-LEVEL TARGETS MET: {'YES! 🎉' if all_met else 'NOT YET 📈'}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No evaluation results available\")\n",
    "\n",
    "print(f\"\\nResults saved to: {Path(OUTPUT_DIR).absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
